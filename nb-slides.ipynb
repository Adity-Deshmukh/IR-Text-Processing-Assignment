{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJbQ76edh3ht",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3><span style=\"color:#1fa2ad\">CS F469: Information Retrieval</span></h3>\n",
    "<h1><b>Text Processing</b></h1>\n",
    "<h2><span style=\"color:#1fa2ad\">Group 11</span></h2>\n",
    "\n",
    "- Aditya Deshmukh\n",
    "- Arshit Modi\n",
    "- Guntaas Singh\n",
    "- Saptarshi Das\n",
    "- Siddarth Agrawal\n",
    "\n",
    "*November 26, 2020*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFy2Iasx6rVH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **PART 1**\n",
    "\n",
    "Part 1 involves building an index over a subset of 89,095 documents from the Wikipedia corpus for processing free-text queries using the vector space model.\n",
    "\n",
    "<h1 style=\"color:#1fa2ad\"><b>Index Construction</b></h1>\n",
    "\n",
    "- *Beautiful Soup* is used to extract the document text. Punctuation is removed and all terms are converted to lower-case. Other metadata, such as the document titles and IDs are also collected and stored.\n",
    "\n",
    "- For fast retrieval, we use an **inverted index** implemented using Python dictionaries.\n",
    "\n",
    "- The simple **in-memory sort-based indexing** algorithm is found to be sufficiently efficient for building the index in a reasonable amount of time.\n",
    "\n",
    "- Additionally, we calculate and store normaization factors for all document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63GO8i4TS708",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains various auxilary functions used throughout the notebook\n",
    "# used during query processing\n",
    "\n",
    "def getTermFreq(freq):\n",
    "  '''\n",
    "    int -> double\n",
    "\n",
    "    Input : term_frequecny\n",
    "    Output : normalized term frequency\n",
    "\n",
    "    Description : \n",
    "    returns the log to the base 10 noramlized value \n",
    "  '''\n",
    "\n",
    "  return 1 + math.log10(freq)\n",
    "\n",
    "def getInvDocFreq(term, index):\n",
    "  '''\n",
    "    string,dict -> double\n",
    "\n",
    "    Input : term,index\n",
    "    Output : inverse document frequency\n",
    "\n",
    "    Description : \n",
    "    returns the inverse document for the term\n",
    "  '''\n",
    "\n",
    "  num_docs = index['num_docs']\n",
    "  doc_freq = len(index['postings'][term])\n",
    "  return math.log10(num_docs / doc_freq)\n",
    "\n",
    "def getTopKDocs(scores, k):\n",
    "  '''\n",
    "    list,int -> list\n",
    "\n",
    "    Input : scores,k\n",
    "    Output : list of documents\n",
    "\n",
    "    Description : \n",
    "    returns a list of top 'k' documents with the highest score\n",
    "  '''\n",
    "\n",
    "  temp=list()\n",
    "\n",
    "  for i in range(len(scores)):\n",
    "      temp.append(i) \n",
    "  for n in range(0,k):\n",
    "      maxi=n\n",
    "      for i in range(n,len(temp)):\n",
    "          if scores[temp[i]]>scores[temp[maxi]]:\n",
    "              maxi=i\n",
    "      temp1=temp[n]\n",
    "      temp[n]=temp[maxi]\n",
    "      temp[maxi]=temp1\n",
    "\n",
    "  return temp[0:k]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjn29xJ43PYW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def buildIndex(corpuspath):\n",
    "  postings = {}\n",
    "  num_docs = 0\n",
    "  norm_factors = []\n",
    "  doc_titles = []\n",
    "  doc_ids = []\n",
    "\n",
    "  i_doc = 0\n",
    "\n",
    "  for foldername in os.listdir(corpuspath):\n",
    "    \n",
    "    # Constructing index from the 'AA' folder only\n",
    "    if foldername != 'AA':\n",
    "      continue\n",
    "    \n",
    "    for filename in os.listdir(os.path.join(corpuspath, foldername)):\n",
    "      filepath = os.path.join(corpuspath, foldername, filename)\n",
    "\n",
    "      # Opening the file in read only mode\n",
    "      with open(filepath,'r',encoding='utf-8') as file:\n",
    "        soup = file.read()\n",
    "      soup = bsoup(soup)\n",
    "      # Extracting all the documents\n",
    "      docs = soup.find_all('doc')\n",
    "      num_docs += len(docs)\n",
    "      # Getting all the doc_titles and doc_ids\n",
    "      doc_titles += [doc.get('title') for doc in docs]\n",
    "      doc_ids += [doc.get('id') for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "        # Removing punctuations from the documents and tokenizing\n",
    "        doctxt = doc.get_text()\n",
    "        doctxt = doctxt.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = word_tokenize(doctxt)\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "\n",
    "        # Getting the term frequency for the term in the document\n",
    "        doc_dict = {}\n",
    "        for token in tokens:\n",
    "          if token in doc_dict:\n",
    "            doc_dict[token]=doc_dict[token]+1\n",
    "          else:\n",
    "            doc_dict[token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "        # Calculating the L2 norm for the doc vector and adding \n",
    "        # the doc id and term frequency to the index\n",
    "        norm_factors.append(0)\n",
    "        for key in doc_dict:\n",
    "          if key in postings:\n",
    "            postings[key].append((i_doc,doc_dict[key]))\n",
    "          else:\n",
    "            postings[key] = [(i_doc, doc_dict[key])]\n",
    "          norm_factors[i_doc] += getTermFreq(doc_dict[key]) ** 2\n",
    "\n",
    "        norm_factors[i_doc] = math.sqrt(norm_factors[i_doc])\n",
    "        i_doc = i_doc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  # Sorting the posting list for each term on the basis of document id\n",
    "  for key in postings:\n",
    "    postings[key] = sorted(postings[key])\n",
    "  \n",
    "  # Generate the index\n",
    "  index = {\n",
    "      'num_docs':num_docs,\n",
    "      'postings':postings,\n",
    "      'norm_factors':norm_factors,\n",
    "      'doc_titles':doc_titles,\n",
    "      'doc_ids':doc_ids\n",
    "  }\n",
    "\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4K0lhlez4-rx",
    "outputId": "49cde276-a3cd-46aa-c9b8-5e02445af8c6",
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Index for the 'AA' folder\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_33\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_46\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_52\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_79\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_59\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_10\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_20\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_07\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_80\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_68\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_67\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_69\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_74\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_30\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_01\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_17\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_23\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_31\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_34\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_53\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_95\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_36\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_58\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_28\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_02\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_43\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_39\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_08\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_75\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_03\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_49\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_90\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_92\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_81\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_57\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_26\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_44\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_99\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_98\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_35\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_61\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_09\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_51\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_12\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_06\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_65\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_25\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_13\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_84\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_78\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_72\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_14\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_62\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_04\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_05\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_45\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_19\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_55\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_73\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_83\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_96\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_54\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_40\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_86\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_87\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_50\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_27\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_15\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_89\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_41\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_00\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_88\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_42\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_97\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_16\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_63\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_38\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_21\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_66\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_71\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_70\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_93\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_85\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_18\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_47\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_48\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_32\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_82\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_76\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_11\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_94\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_64\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_77\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_29\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_22\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_60\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_24\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_91\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_56\n",
      "\tProcessing /content/drive/My Drive/IR assignment/corpus/Wikipedia/AA/wiki_37\n",
      "Generated the index_AA in 0 hrs, 23 mins, 41 secs\n"
     ]
    }
   ],
   "source": [
    "# Building the index\n",
    "index = buildIndex('/content/drive/My Drive/IR assignment/corpus/Wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ_z12GC5FFA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling the index\n",
    "pickle.dump(index, open('/content/drive/My Drive/IR assignment/index_AA.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kur6HohD6CuG",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Loads the generated index\n",
    "index = pickle.load(open('/content/drive/My Drive/IR assignment/index_AA.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW2vIafiioBv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"color:#1fa2ad\"><b>Query Processing</b></h1>\n",
    "\n",
    "- First, we convert the query to lowercase, remove punctuations and tokenize the query. \n",
    "- We calculate the cosine similarity score for the query vector (following *ltc* scheme) with every document vector (following *lnc* scheme).\n",
    "- The top-K documents (K=10) are returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duL3kDEXLOPc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def processQuery(query, index, k):\n",
    "\n",
    "  scores = [0] * index['num_docs']\n",
    "  query_freq = {}\n",
    "\n",
    "  # Tokenize the query after removing punctuations and converting to lowercase\n",
    "  query = query.translate(str.maketrans('','',string.punctuation))\n",
    "  tokens = word_tokenize(query)\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "\n",
    "  # Find term frequency for the query\n",
    "  for term in tokens:\n",
    "    if term in query_freq:\n",
    "      query_freq[term] += 1\n",
    "    else:\n",
    "      query_freq[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  # Calculating cosine similarity with certain optimizations:\n",
    "    # Terms absent in the query do not contribute to the score, \n",
    "        # and are hence, ignored.\n",
    "    # Document vector normalization factors computed duing index construction\n",
    "        # are directly used.\n",
    "  query_norm = 0\n",
    "  for term in query_freq:\n",
    "    if term not in index['postings']:\n",
    "      continue\n",
    "    queryWeight = getTermFreq(query_freq[term]) * getInvDocFreq(term, index)\n",
    "    query_norm += queryWeight ** 2\n",
    "    for i_doc, docFreq in index['postings'][term]:\n",
    "      docWeight = getTermFreq(docFreq)\n",
    "      scores[i_doc] += queryWeight * docWeight\n",
    "  query_norm = math.sqrt(query_norm)\n",
    "  \n",
    "  for i_doc, norm_factor in enumerate(index['norm_factors']):\n",
    "    scores[i_doc] /= (norm_factor * query_norm)\n",
    "\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgxMObMA53tD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def testQuery(index, query):\n",
    "    \n",
    "  print(f'Retrieving documents for the query : {query} ...')\n",
    "\n",
    "  # k to specify number of results to get\n",
    "  k = 10\n",
    "  query = query.lower()\n",
    "  start = time()\n",
    "  scores = processQuery(query, index, k)\n",
    "  res = getTopKDocs(scores, k)\n",
    "  end = time()\n",
    "  print(f\"Query processing took {end-start:.3f} sec.\")\n",
    "\n",
    "  # Printing the results\n",
    "  print('Printing retrieved documents ... ')\n",
    "  for rank, i_doc in enumerate(res):\n",
    "    doc_freq = -1\n",
    "    print(f\"{rank+1:3}: [DocID : {i_doc:5}]  \" + \n",
    "          \"DocTitle : {index['doc_titles'][i_doc]:35}  \" + \n",
    "          \"Score : {scores[i_doc]:.3f})\"\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0VzVpj6-9La",
    "outputId": "5e0a6a3e-f284-402c-cb7f-c23bb1f0a509",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the query to be searched : animal cell\n",
      "Retrieving documents for the query : animal cell ...\n",
      "Query processing took 0.104 sec.\n",
      "Printing retrieved documents ... \n",
      "  1: [DocID : 61241] DocTitle : K cell                              (Score : 0.388)\n",
      "  2: [DocID : 80157] DocTitle : Cell potential                      (Score : 0.388)\n",
      "  3: [DocID : 58583] DocTitle : Animal (disambiguation)             (Score : 0.212)\n",
      "  4: [DocID : 35211] DocTitle : Cell Cycle (journal)                (Score : 0.210)\n",
      "  5: [DocID : 19815] DocTitle : Amphibian (disambiguation)          (Score : 0.199)\n",
      "  6: [DocID : 20277] DocTitle : Flora (disambiguation)              (Score : 0.160)\n",
      "  7: [DocID : 34066] DocTitle : Eight-cell stage                    (Score : 0.147)\n",
      "  8: [DocID :  9588] DocTitle : Plasmolysis                         (Score : 0.145)\n",
      "  9: [DocID : 86094] DocTitle : Mannan                              (Score : 0.141)\n",
      " 10: [DocID : 72645] DocTitle : Zygote                              (Score : 0.118)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the query\n",
    "query = input(\"Please enter the query to be searched : \") # animal cell\n",
    "testQuery(index, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At2L3n1D_Bok",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **PART 2**\n",
    "\n",
    "We now focus on improving the performance of our model by using heuristics like title-weighing and query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvNJDPF6_PGh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"color:#1fa2ad\"><b>Improvement : Title Weighing</b></h1>\n",
    "\n",
    "\n",
    "- The title information supplied in the documents can be very useful while retrieving the documents as it is a solid indicator of information need presented by the user in terms of query. \n",
    "- We take into account by calculating the **jaccard similarity** of document title and query and merging it with the cosine score using a suitable parameter.\n",
    "- By the method of trial and error, we ran our experiment for various values of lambda, and found that **lambda = 0.5** works well for our model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZRjMTOK_OWP",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(title,query):\n",
    "\n",
    "  # Tokenizing the document title and query\n",
    "  l1=regexp_tokenize(title.lower(), \"[\\w']+\")\n",
    "  l2=regexp_tokenize(query.lower(), \"[\\w']+\")\n",
    "\n",
    "  s1=set()\n",
    "  s2=set()\n",
    "\n",
    "  # Count for storing the intersection of title and query terms\n",
    "  count=0\n",
    "  for i in l1:\n",
    "    s1.add(i)\n",
    "    s2.add(i)\n",
    "  \n",
    "  for i in l2:\n",
    "    if i in s1:\n",
    "        s1.remove(i)\n",
    "        count=count+1\n",
    "    s2.add(i)\n",
    "  \n",
    "  return count/len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi3ECeiv_AUA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def processQuery(query, index, k, lambda1):\n",
    "\n",
    "  # Tokenize the query after removing punctuations\n",
    "  query = query.translate(str.maketrans('','',string.punctuation))\n",
    "  scores = [0] * index['num_docs']\n",
    "  query_freq = {}\n",
    "  tokens = word_tokenize(query)\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  \n",
    "  # Calculate frequency for query terms\n",
    "  for term in tokens:\n",
    "    if term in query_freq:\n",
    "      query_freq[term] += 1\n",
    "    else:\n",
    "      query_freq[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  # Calculating cosine similarity and adding an extra lamda * jaccard \n",
    "  # similarity between document title and query-\n",
    "  query_norm = 0\n",
    "  for term in query_freq:\n",
    "    if term not in index['postings']:\n",
    "      continue\n",
    "    queryWeight = getTermFreq(query_freq[term]) * getInvDocFreq(term, index)\n",
    "    query_norm += queryWeight ** 2\n",
    "    for i_doc, docFreq in index['postings'][term]:\n",
    "      docWeight = getTermFreq(docFreq)\n",
    "      scores[i_doc] += queryWeight * docWeight\n",
    "  query_norm = math.sqrt(query_norm)\n",
    "\n",
    "  for i_doc, norm_factor in enumerate(index['norm_factors']):\n",
    "    scores[i_doc] /= (norm_factor * query_norm)\n",
    "    scores[i_doc]=scores[i_doc]+jaccard(index['doc_titles'][i_doc],query)*lambda1\n",
    "  \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmiUUucPCb5Z",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Lamda_title_weigh is the factor by which we want to weigh in the relevance \n",
    "# between document title and query\n",
    "lambda_title_weigh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the query\n",
    "query = input(\"Please enter the query to be searched : \")\n",
    "testQuery(index ,query , lambda_title_weigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxLahKg8CfqK",
    "outputId": "aaec8403-a18b-4602-fbe3-4c786d6f770b",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the query to be searched : Teenage mutant ninja Turtles\n",
      "Retrieving documents for the query : Teenage mutant ninja Turtles using title weighing ...\n",
      "Query processing took 0.786 sec.\n",
      "Printing retrieved documents ... \n",
      "  1: [DocID : 88893] DocTitle : Teenage Mutant Ninja Turtles        (Score : 0.599)\n",
      "  2: [DocID : 21221] DocTitle : Ninja                               (Score : 0.186)\n",
      "  3: [DocID : 19779] DocTitle : Belgian hip hop                     (Score : 0.145)\n",
      "  4: [DocID :  4593] DocTitle : Ninja Tune                          (Score : 0.128)\n",
      "  5: [DocID :  6019] DocTitle : High Falls, New York                (Score : 0.115)\n",
      "  6: [DocID : 39720] DocTitle : Atari Teenage Riot                  (Score : 0.107)\n",
      "  7: [DocID : 47398] DocTitle : Sean Astin                          (Score : 0.083)\n",
      "  8: [DocID : 26063] DocTitle : Mae Whitman                         (Score : 0.079)\n",
      "  9: [DocID : 43634] DocTitle : Corey Feldman                       (Score : 0.077)\n",
      " 10: [DocID : 21424] DocTitle : Parallax scrolling                  (Score : 0.077)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the query\n",
    "query = input(\"Please enter the query to be searched : \")\n",
    "testQuery(index ,query , lambda_title_weigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUXW586ECvNA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"color:#1fa2ad\"><b>Improvement : Automatic Query Expansion</b></h1>\n",
    "\n",
    "- **Word mismatch**: Users of IR systems often use different words to describe a concept in their queries than those used in the documents. Our IR system ignores such terms, which can negatively affect the ranking performance.\n",
    "- Since the meaning of a word can be heavily context-dependent, we cannot depend on the synonyms as much as the original query terms. \n",
    "- To this end, we assign a weight (discount_rate)(adjustable parameter, less than 1) to each synonym added to the query, using which the terms are weighed when calculating the cosine similarity.\n",
    "- By the method of trial and error, we ran our experiment for various values of discount_rate and number of synonyms (expansion_factor), and found that **discount_rate = 0.7** and **expansion_factor = 5** works well for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOSlqFYCDeAf",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def expandTokenList(token_list, mask, index, expansion_factor=5, \n",
    "                    always_expand=True, discount_rate=0.7):\n",
    "\n",
    "  exp_token_list = token_list.copy()\n",
    "\n",
    "  for org_word in token_list:\n",
    "    syn_count = 0\n",
    "\n",
    "    # if the word is found in the index do not expand query corresponding to the \n",
    "    # term and check also check whether query expansion is true or not\n",
    "    if org_word in index['postings'] and not always_expand:\n",
    "      continue\n",
    "\n",
    "    # Get lemma for the query term (equal to synset name)\n",
    "    for synset in wordnet.synsets(org_word):\n",
    "      lemma = synset.lemmas()[0] \n",
    "\n",
    "      # add synonym to the new list if the not already present in the list,\n",
    "      # synonym should be present in the index and number of synonyms extracted\n",
    "      # should be less than expansion factor\n",
    "      if (syn_count < expansion_factor) and \n",
    "            (lemma.name().lower() not in exp_token_list) and \n",
    "            (lemma.name().lower() in index['postings']):\n",
    "        exp_token_list.append(lemma.name().lower())\n",
    "        mask.append(discount_rate)\n",
    "        syn_count += 1\n",
    "\n",
    "  return exp_token_list, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmfuRGp1DpwX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def processQuery(query, index, k, queryExpansion=False):\n",
    "  \n",
    "  # Tokenize the query after removing punctuations\n",
    "  query = query.translate(str.maketrans('','',string.punctuation))\n",
    "  scores = [0] * index['num_docs']\n",
    "  query_freq = {}\n",
    "  mask_dict = {}\n",
    "  tokens = word_tokenize(query)\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  mask = [1] * len(tokens)\n",
    "\n",
    "  # Call query expansion\n",
    "  if queryExpansion:\n",
    "    tokens, mask = expandTokenList(\n",
    "        tokens, mask, index, expansion_factor=5, \n",
    "        always_expand=True, discount_rate=0.7\n",
    "    )\n",
    "    print(f\"Expanded query: {str(tokens)}\")\n",
    "\n",
    "  # Compute frequency of query terms\n",
    "  for i in range(len(tokens)):\n",
    "    term = tokens[i]\n",
    "    m_val = mask[i]\n",
    "\n",
    "    if term in query_freq:\n",
    "      query_freq[term] += 1\n",
    "      mask_dict[term] = max(mask_dict[term], m_val)\n",
    "    else:\n",
    "      query_freq[term] = 1\n",
    "      mask_dict[term] = m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  # Calculating cosine similarity and also introduced a mask factor which (generally)\n",
    "  # reduces the weight of the synonyms taken into consideration\n",
    "  query_norm = 0\n",
    "  for term in query_freq:\n",
    "\n",
    "    if term not in index['postings']:\n",
    "      print(f\"DEBUG: Corpus missing term '{term}'\")\n",
    "      continue\n",
    "\n",
    "    queryWeight = getTermFreq(query_freq[term]) * getInvDocFreq(term, index)\n",
    "    query_norm += queryWeight ** 2\n",
    "    for i_doc, docFreq in index['postings'][term]:\n",
    "      docWeight = getTermFreq(docFreq)\n",
    "      scores[i_doc] += queryWeight * docWeight * mask_dict[term]\n",
    "  query_norm = math.sqrt(query_norm)\n",
    "  \n",
    "  for i_doc, norm_factor in enumerate(index['norm_factors']):\n",
    "    scores[i_doc] /= (norm_factor * query_norm)\n",
    "  \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTesnxWyDthG",
    "outputId": "5467fec0-a4a8-4a33-f845-5c612e8bbd1d",
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the query to be searched : competition\n",
      "Retrieving documents for the query : competition using query expansion ...\n",
      "Expanded query: ['competition', 'contest', 'rival']\n",
      "Query processing took 0.110 sec.\n",
      "Printing retrieved documents ... \n",
      "  1: [DocID : 63439] DocTitle : Imperfect competition               (Score : 0.149)\n",
      "  2: [DocID : 32350] DocTitle : Snowboarding at the 2002 Winter Olympics (Score : 0.114)\n",
      "  3: [DocID : 42993] DocTitle : Interactive Fiction Competition     (Score : 0.105)\n",
      "  4: [DocID : 50968] DocTitle : Gandalf (theorem prover)            (Score : 0.090)\n",
      "  5: [DocID : 32346] DocTitle : Luge at the 2002 Winter Olympics    (Score : 0.088)\n",
      "  6: [DocID : 84232] DocTitle : International Obfuscated C Code Contest (Score : 0.082)\n",
      "  7: [DocID : 15654] DocTitle : Eurovision Song Contest 1956        (Score : 0.078)\n",
      "  8: [DocID : 86759] DocTitle : Miss World                          (Score : 0.077)\n",
      "  9: [DocID : 32352] DocTitle : Bobsleigh at the 2002 Winter Olympics (Score : 0.075)\n",
      " 10: [DocID :  4879] DocTitle : List of Olympic medalists in athletics (men) (Score : 0.074)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the query\n",
    "query = input(\"Please enter the query to be searched : \")\n",
    "testQuery(index ,query ,queryExpansion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"color:#1fa2ad\"><b>Thank you</b></h1>\n",
    "<h2 style=\"color:#1fa2ad\"><b>Contributions</b></h2>\n",
    "\n",
    "| Task | Aditya Deshmukh | Arshit Modi | Guntaas Singh | Saptarshi Das | Siddarth Agrawal |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Index Construction  | X   |  |  | X   |  |\n",
    "| Query Processing    |  |  | X   | | |\n",
    "| Query Evaluation    | | X   |  |  | X   |\n",
    "| I1: Title Weighing  | | X |  |  | X   |\n",
    "| I2: Query Expansion | X   |  | X   |  | |\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "IR assignment final submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
